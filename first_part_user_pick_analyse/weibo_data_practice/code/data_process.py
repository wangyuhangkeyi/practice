import pandas as pd
import re
import csv
import os

# ğŸ“ è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…è·¯å¾„ä¿®æ”¹ï¼‰
import os
script_dir = os.path.dirname(os.path.abspath(__file__))
file_path = os.path.join(script_dir, '..', 'result', 'tweets_with_comments.csv')

# ğŸ” è·å–æ–‡ä»¶æ‰€åœ¨ç›®å½•
file_dir = os.path.dirname(file_path)

# âœ¨ å®šä¹‰æ¸…æ´—å‡½æ•°
def clean_text(text):
    if not isinstance(text, str):
        return ""
    
    # åˆ é™¤æˆå¯¹å‡ºç°çš„ #ï¼Œä¿ç•™ä¸­é—´çš„å†…å®¹ï¼ˆå¦‚ #èåœå¿«è·‘# â†’ èåœå¿«è·‘ï¼‰
    text = re.sub(r'#([^#\n\r]+?)#', r'\1', text)
    
    # åˆ é™¤â€œå±•å¼€â€å’Œâ€œè§†é¢‘â€
    text = text.replace("å±•å¼€", "").replace("è§†é¢‘", "").replace("c","").replace("å›å¤","")
    
    # åˆ é™¤å­¤ç«‹çš„ #
    text = text.replace("#", "")
    
    return text.strip()

# ğŸ“„ ä½¿ç”¨ pandas è‡ªåŠ¨è¯†åˆ«åˆ†éš”ç¬¦ï¼Œå¹¶è·³è¿‡é”™è¯¯è¡Œ
df = pd.read_csv(file_path, on_bad_lines='skip')

# æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
required_columns = ['Tweet', 'Comments', 'Likes']
if not all(col in df.columns for col in required_columns):
    raise ValueError(f"âŒ CSV æ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦åˆ— {required_columns}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")

# âœ… åˆå§‹åŒ–åˆ—è¡¨ç”¨äºå­˜å‚¨æ¸…æ´—åçš„æ•°æ®
cleaned_rows = []

# ğŸ”§ ç¬¬ä¸€æ­¥ï¼šç»Ÿä¸€è¯„è®ºæ ¼å¼ï¼ˆå¢å¼ºå¤„ç†é€»è¾‘ï¼‰
for index, row in df.iterrows():
    tweet = clean_text(row['Tweet'])  # æ¸…æ´—å¾®åšå†…å®¹
    comment_block = clean_text(row['Comments'])  # æ¸…æ´—è¯„è®ºå—
    likes = row['Likes']  # è·å–ç‚¹èµæ•°

    if pd.isna(comment_block):
        cleaned_rows.append([tweet, "", likes])
        continue

    # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    comment_block = str(comment_block).strip()

    # æŒ‰è¡Œåˆ†å‰²
    lines = [line.strip() for line in comment_block.split('\n')]

    unified_comments = []
    i = 0
    while i < len(lines):
        line = lines[i]

        # åˆ¤æ–­æ˜¯å¦æ˜¯ä¸€ä¸ªå¯èƒ½çš„è¯„è®ºIDï¼ˆä¸å«å†’å·ï¼Œéç©ºï¼‰
        if line and ':' not in line and 'ï¼š' not in line:

            # çœ‹ä¸‹ä¸€è¡Œæ˜¯å¦æ˜¯ä»¥å†’å·å¼€å¤´çš„å†…å®¹
            if i + 1 < len(lines) and (lines[i+1].startswith(':') or lines[i+1].startswith('ï¼š')):
                content_line = lines[i+1][1:].strip()  # å»æ‰å†’å·
                unified_comments.append(f"{line}ï¼š{content_line}")
                i += 2  # è·³è¿‡è¿™ä¸¤è¡Œ
            else:
                unified_comments.append(line)
                i += 1
        else:
            unified_comments.append(line)
            i += 1

    # åˆå¹¶æˆå­—ç¬¦ä¸²ï¼Œæ¯æ¡è¯„è®ºä¸€è¡Œ
    unified_comments_str = '\n'.join(unified_comments)
    
    cleaned_rows.append([tweet, unified_comments_str, likes])

# ğŸ“¥ ä¿å­˜ä¸­é—´æ¸…æ´—ç»“æœï¼Œæ–¹ä¾¿äººå·¥æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®
cleaned_df = pd.DataFrame(cleaned_rows, columns=['å¾®åšå†…å®¹', 'ç»Ÿä¸€åçš„è¯„è®º', 'ç‚¹èµæ•°'])
cleaned_output_file = os.path.join(file_dir, 'cleaned_tweets.csv')
cleaned_df.to_csv(cleaned_output_file, index=False, encoding='utf-8-sig')
print(f"âœ… å·²ä¿å­˜æ¸…æ´—åçš„ä¸­é—´æ•°æ®è‡³ {cleaned_output_file}")

# ğŸ“Š ç»Ÿè®¡åŸå§‹æ•°æ®ä¸­çš„å¾®åšæ•°é‡å’Œè¯„è®ºæ€»æ•°
total_weibo_count = len(df)
total_comment_count = df['Comments'].dropna().apply(lambda x: len(str(x).split('\n'))).sum()
print(f"\nğŸ“Š åˆæ­¥å¤„ç†å®Œæˆï¼š")
print(f" - åŸå§‹å¾®åšæ€»æ•°ï¼š{total_weibo_count}")
print(f" - åŸå§‹è¯„è®ºæ€»æ•°ï¼š{total_comment_count}")

# ğŸ§  ç¬¬äºŒæ­¥ï¼šåˆå¹¶é‡å¤å¾®åšå†…å®¹ + åˆ†ç±»è¯„è®º + ä¿ç•™æœ€å¤§ç‚¹èµæ•° + è¯„è®ºå»é‡ä¼˜å…ˆä¿ç•™å¸¦ç‚¹èµæ•°çš„è¯„è®º
processed_data_dict = {}

# ğŸ” å®šä¹‰åˆ¤æ–­æ˜¯å¦ä¸è¯é¢˜ç›¸å…³çš„å‡½æ•°
def is_related_to_topic(text, topic_keywords):
    text = text.lower()
    for keyword in topic_keywords:
        if keyword.lower() in text:
            return True
    return False

# ğŸ“‹ å®šä¹‰å…³é”®è¯
topic_keywords = ['èåœå¿«è·‘', 'Robotaxi', 'æ— äººè½¦', 'è‡ªåŠ¨é©¾é©¶å‡ºç§Ÿè½¦', 'æ— äººé©¾é©¶', 'è½¦', 'å®ƒ','é©¾','å¸','ä¹˜','å°‘','å¤š']

# ğŸ”„ éå†æ¸…æ´—åçš„æ•°æ®è¿›è¡Œåç»­å¤„ç†
for index, row in cleaned_df.iterrows():
    content = clean_text(row['å¾®åšå†…å®¹'])  # å†æ¬¡æ¸…æ´—å¾®åšå†…å®¹
    comments = row['ç»Ÿä¸€åçš„è¯„è®º']
    likes = row['ç‚¹èµæ•°']

    if pd.isna(comments):
        continue

    comment_lines = comments.split('\n')

    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡é‡åˆ°è¯¥å¾®åšå†…å®¹ï¼Œåˆ™ç›´æ¥åˆå§‹åŒ–
    if content not in processed_data_dict:
        processed_data_dict[content] = {
            'related': {},  # key=(cid, ct)ï¼Œvalue=(line, likes)
            'unrelated': {},
            'likes': likes
        }
    else:
        # å¦‚æœå·²ç»å­˜åœ¨ï¼Œå°è¯•æ¯”è¾ƒç‚¹èµæ•°ï¼Œä¿ç•™è¾ƒå¤§çš„é‚£ä¸ª
        current_likes = processed_data_dict[content]['likes']
        try:
            new_likes_num = float(likes) if pd.notna(likes) else 0
            curr_likes_num = float(current_likes) if pd.notna(current_likes) else 0
            if new_likes_num > curr_likes_num:
                processed_data_dict[content]['likes'] = likes
        except:
            pass

    related_comments = processed_data_dict[content]['related']
    unrelated_comments = processed_data_dict[content]['unrelated']

    for line in comment_lines:
        if not line.strip():
            continue

        try:
            # åˆ†å‰²è¯„è®ºIDå’Œå†…å®¹
            parts = line.split('ï¼š', 1)
            if len(parts) < 2:
                continue  # è·³è¿‡ä¸åˆæ³•æ ¼å¼
            comment_id, raw_comment = parts
            comment_id = comment_id.strip()
            raw_comment = raw_comment.strip()

            # æå–ç‚¹èµæ•°
            like_match = re.search(r'\(ç‚¹èµæ•°\s*[:ï¼š]\s*(\d+)\)$', raw_comment)
            comment_likes = int(like_match.group(1)) if like_match else 0

            # å»æ‰ç‚¹èµæ•°å­—æ®µåçš„å†…å®¹
            clean_comment = re.sub(r'\s*\(\s*ç‚¹èµæ•°\s*[:ï¼š]\s*\d+\s*\)$', '', raw_comment).strip()

            full_key = (comment_id, clean_comment)

            # åˆ¤æ–­æ˜¯å¦ç›¸å…³
            is_related = len(clean_comment) > 2 and is_related_to_topic(clean_comment, topic_keywords)

            target_dict = related_comments if is_related else unrelated_comments

            if full_key in target_dict:
                existing_line, existing_likes = target_dict[full_key]

                # åªæœ‰å½“å‰è¯„è®ºæœ‰ç‚¹èµæ•°ã€ä¸”å·²æœ‰è¯„è®ºæ— ç‚¹èµæ•°ï¼Œæˆ–ç‚¹èµæ›´é«˜æ‰æ›¿æ¢
                if (comment_likes > 0 and existing_likes == 0) or (comment_likes > existing_likes):
                    target_dict[full_key] = (f"{comment_id}ï¼š{raw_comment}", comment_likes)
            else:
                # ç¬¬ä¸€æ¬¡å‡ºç°ï¼Œæ­£å¸¸åŠ å…¥
                target_dict[full_key] = (f"{comment_id}ï¼š{raw_comment}", comment_likes)

        except Exception as e:
            print(f"âš ï¸ æ— æ³•è§£æçš„è¯„è®ºè¡Œ: {line} | é”™è¯¯: {e}")

# âœ… å®šä¹‰å»é‡å‡½æ•°ï¼šä¿ç•™å«ç‚¹èµæ•°çš„è¯„è®º
def deduplicate_comments(comments_list):
    """
    å¯¹è¯„è®ºåˆ—è¡¨è¿›è¡Œå»é‡ï¼Œç›¸åŒå†…å®¹ä¼˜å…ˆä¿ç•™å¸¦ç‚¹èµæ•°çš„è¯„è®ºã€‚
    :param comments_list: åŸå§‹è¯„è®ºåˆ—è¡¨ï¼ˆæ ¼å¼ä¸º 'ç”¨æˆ·åï¼šè¯„è®ºå†…å®¹ (ç‚¹èµæ•°: 123)'ï¼‰
    :return: å»é‡åçš„è¯„è®ºåˆ—è¡¨
    """
    comment_dict = {}  # key: (comment_id, clean_content)ï¼Œvalue: (full_line, like_count)

    for line in comments_list:
        if not line.strip():
            continue

        try:
            cid, raw_comment = line.split('ï¼š', 1)
            cid = cid.strip()
            raw_comment = raw_comment.strip()

            like_match = re.search(r'\(ç‚¹èµæ•°\s*[:ï¼š]\s*(\d+)\)$', raw_comment)
            likes = int(like_match.group(1)) if like_match else 0

            clean_comment = re.sub(r'\s*\(\s*ç‚¹èµæ•°\s*[:ï¼š]\s*\d+\s*\)$', '', raw_comment).strip()

            key = (cid, clean_comment)

            if key not in comment_dict:
                comment_dict[key] = (line, likes)
            else:
                existing_line, existing_likes = comment_dict[key]
                if (likes > 0 and existing_likes == 0) or (likes > existing_likes):
                    comment_dict[key] = (line, likes)

        except Exception as e:
            print(f"âš ï¸ æ— æ³•è§£æçš„è¯„è®ºè¡Œ: {line} | é”™è¯¯: {e}")

    return [v[0] for v in comment_dict.values()]

# å®šä¹‰åå¤„ç†å‡½æ•°ï¼šç²¾ç»†åŒ–å»é‡
def post_process_comments(comments_block):
    """
    å¯¹è¯„è®ºå—è¿›è¡Œç²¾ç»†åŒ–å»é‡ï¼š
    - ç›¸åŒç”¨æˆ· + ç›¸åŒè¯„è®ºå†…å®¹è§†ä¸ºé‡å¤ï¼ˆå¿½ç•¥æ˜¯å¦å¸¦ç‚¹èµæ•°ï¼‰
    - è‹¥åªæœ‰ä¸€æ¡å¸¦ç‚¹èµæ•°ï¼Œåˆ™ä¿ç•™å¸¦ç‚¹èµæ•°çš„é‚£æ¡
    - è‹¥éƒ½å¸¦ç‚¹èµæ•°ï¼Œåˆ™ä¿ç•™ç‚¹èµæ•°æ›´é«˜çš„é‚£æ¡
    :param comments_block: å¤šæ¡è¯„è®ºç»„æˆçš„å­—ç¬¦ä¸²ï¼Œæ¯è¡Œä¸€æ¡è¯„è®º
    :return: å»é‡åçš„è¯„è®ºåˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
    """
    lines = [line.strip() for line in comments_block.split('\n') if line.strip()]
    seen = {}  # key: (comment_id, clean_content) -> value: (full_line, likes)

    for line in lines:
        try:
            cid, raw_comment = line.split('ï¼š', 1)
            cid = cid.strip()
            raw_comment = raw_comment.strip()

            # æå–ç‚¹èµæ•°
            like_match = re.search(r'\(ç‚¹èµæ•°\s*[:ï¼š]\s*(\d+)\)$', raw_comment)
            likes = int(like_match.group(1)) if like_match else 0

            # æ¸…æ´—æ‰ç‚¹èµå­—æ®µä½œä¸º clean_content
            clean_comment = re.sub(r'\s*\(\s*ç‚¹èµæ•°\s*[:ï¼š]\s*\d+\s*\)$', '', raw_comment).strip()

            key = (cid, clean_comment)

            if key not in seen:
                seen[key] = (line, likes)
            else:
                existing_line, existing_likes = seen[key]

                # åªè¦å½“å‰è¿™æ¡æœ‰ç‚¹èµä¸”æ›´é«˜ï¼Œå°±æ›¿æ¢
                if (likes > 0 and existing_likes == 0) or (likes > existing_likes):
                    seen[key] = (line, likes)

        except Exception as e:
            print(f"âš ï¸ åå¤„ç†æ—¶æ— æ³•è§£æè¯„è®ºè¡Œ: {line} | é”™è¯¯: {e}")

    return [v[0] for v in seen.values()]

# ğŸ“¤ æ„å»ºæœ€ç»ˆè¾“å‡ºæ•°æ®
processed_data = []

# éå†æ‰€æœ‰åˆå¹¶åçš„å¾®åšå†…å®¹
for content, data in processed_data_dict.items():
    # ç¬¬ä¸€é˜¶æ®µå»é‡ï¼ˆåŸºäº key: (cid, comment) + likesï¼‰
    related_lines = deduplicate_comments([line for line, _ in data['related'].values()])
    unrelated_lines = deduplicate_comments([line for line, _ in data['unrelated'].values()])

    # å°†å…¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²å—
    related_block = '\n'.join(related_lines)
    unrelated_block = '\n'.join(unrelated_lines)

    # ç¬¬äºŒé˜¶æ®µå»é‡ï¼ˆåå¤„ç†ï¼Œç¡®ä¿æœ€ç»ˆæ ¼å¼ç»Ÿä¸€ã€å½»åº•å»é‡ï¼‰
    final_related = post_process_comments(related_block)
    final_unrelated = post_process_comments(unrelated_block)

    # è½¬å›å­—ç¬¦ä¸²å½¢å¼
    related_str = '\n'.join(final_related)
    unrelated_str = '\n'.join(final_unrelated)
    likes = data['likes']

    processed_data.append([content, related_str, unrelated_str, likes])

# ğŸ“Š ç»Ÿè®¡å¤„ç†åå¾®åšæ•°é‡ å’Œ æ€»è¯„è®ºæ•°ï¼ˆç›¸å…³ + æ— å…³ï¼‰
processed_weibo_count = len(processed_data)

# ğŸ‘‡ è®¡ç®—æ‰€æœ‰è¯„è®ºæ€»æ•°
total_related_comments = sum(len([x for x in item[1].split('\n') if x.strip()]) for item in processed_data)
total_unrelated_comments = sum(len([x for x in item[2].split('\n') if x.strip()]) for item in processed_data)
total_processed_comments = total_related_comments + total_unrelated_comments

# âœ… æœ€ç»ˆè¾“å‡ºæ–‡ä»¶ä¹Ÿä¿å­˜åˆ°åŸå§‹æ–‡ä»¶ç›®å½•ä¸‹
final_output_file = os.path.join(file_dir, 'processed_tweets_final.csv')
with open(final_output_file, mode='w', newline='', encoding='utf-8-sig', errors='ignore') as f:
    writer = csv.writer(f, quoting=csv.QUOTE_ALL, escapechar='\\')
    writer.writerow(['å¾®åšå†…å®¹', 'ä¸è¯é¢˜ç›¸å…³çš„è¯„è®º', 'ä¸è¯é¢˜æ— å…³çš„è¯„è®º', 'ç‚¹èµæ•°'])
    writer.writerows(processed_data)

print(f"\nğŸ“Š æ‰€æœ‰å¤„ç†å®Œæˆï¼š")
print(f" - å¤„ç†åå¾®åšæ€»æ•°ï¼š{processed_weibo_count}")
print(f" - å¤„ç†åè¯„è®ºæ€»æ•°ï¼š{total_processed_comments}")
print(f"âœ… æœ€ç»ˆå¤„ç†ç»“æœå·²ä¿å­˜è‡³ {final_output_file}")