import os
from pathlib import Path
import pandas as pd
import re
import jieba
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim import corpora, models

# -----------------------------
# ğŸ“ è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒï¼ˆè§£å†³ç»˜å›¾ä¸­æ–‡ä¹±ç ï¼‰
# -----------------------------
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# -----------------------------
# ğŸ§  æ‰‹åŠ¨å®šä¹‰ id2label / label2idï¼ˆé€‚é…æœ¬åœ°æƒ…æ„Ÿæ¨¡å‹ï¼‰
# -----------------------------
id2label = {0: "negative", 1: "positive"}
label2id = {"negative": 0, "positive": 1}

# -----------------------------
# ğŸ“ æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
# -----------------------------
script_dir = Path(__file__).resolve().parent
project_root = script_dir.parents[2]
# æ¨¡å‹è·¯å¾„ï¼šä¼˜å…ˆç¯å¢ƒå˜é‡ï¼Œå¦åˆ™ä½¿ç”¨ä»“åº“ models/sentiment
default_model_path = project_root / 'models' / 'sentiment'
model_path = Path(os.environ.get('SENTIMENT_MODEL_PATH', str(default_model_path)))

# -----------------------------
# ğŸ§  åŠ è½½æœ¬åœ° HuggingFace ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹
# -----------------------------
print("ğŸ”„ æ­£åœ¨åŠ è½½ HuggingFace ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆæœ¬åœ°ï¼‰...")

try:
    tokenizer = AutoTokenizer.from_pretrained(str(model_path), local_files_only=True)
    model = AutoModelForSequenceClassification.from_pretrained(
        str(model_path),
        local_files_only=True,
        id2label=id2label,
        label2id=label2id
    )
    classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)

except Exception as e:
    print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ï¼š{e}")
    exit(1)

# -----------------------------
# ğŸ§  ä¸­æ–‡æƒ…æ„Ÿé¢„æµ‹å‡½æ•°
# -----------------------------
def predict_sentiment(text):
    if not isinstance(text, str) or len(text.strip()) == 0:
        return 'ä¸­æ€§', 0.0
    result = classifier(text, truncation=True, max_length=512)[0]
    label = result['label']
    score = result['score']
    sentiment_label = 'æ­£é¢' if label == 'positive' else 'è´Ÿé¢'
    return sentiment_label, score

# -----------------------------
# ğŸ§¾ ä¸­æ–‡åˆ†è¯å‡½æ•°
# -----------------------------
def chinese_tokenize(text):
    return [word for word in jieba.cut(str(text)) if len(word.strip()) > 1]

# -----------------------------
# ğŸ§  LDA ä¸»é¢˜å»ºæ¨¡å‡½æ•°
# -----------------------------
def perform_lda(texts, num_topics=10):
    tokenized_texts = [chinese_tokenize(text) for text in texts]
    dictionary = corpora.Dictionary(tokenized_texts)
    corpus = [dictionary.doc2bow(text) for text in tokenized_texts]

    lda_model = models.LdaModel(
        corpus=corpus,
        id2word=dictionary,
        num_topics=num_topics,
        alpha='auto',
        eta=0.01,
        passes=10
    )

    def get_topic_distribution(bow):
        topics = lda_model.get_document_topics(bow)
        topic_dist = [0] * num_topics
        for t_id, prob in topics:
            topic_dist[t_id] = prob
        return topic_dist

    return lda_model, [get_topic_distribution(bow) for bow in corpus]

# -----------------------------
# ğŸ”‘ TF-IDF å…³é”®è¯æå–
# -----------------------------
def extract_keywords_tfidf(texts, top_n=20):
    valid_texts = [str(text) for text in texts if pd.notna(text) and str(text).strip() != ""]
    if not valid_texts:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬å¯ç”¨äºå…³é”®è¯æå–")
        return []

    vectorizer = TfidfVectorizer(tokenizer=chinese_tokenize, stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(valid_texts)
    feature_array = vectorizer.get_feature_names_out()
    tfidf_sorting = tfidf_matrix.toarray().sum(axis=0).argsort()[::-1]
    top_keywords = [feature_array[i] for i in tfidf_sorting[:top_n]]
    return top_keywords

# -----------------------------
# ğŸ§¹ å»é™¤è¯„è®ºå‰ç¼€ ID å‡½æ•°
# -----------------------------
def clean_comment_prefix(text):
    if not isinstance(text, str):
        return ""
    match = re.match(r'^[^ï¼š]+ï¼š(.+)$', text.strip())
    if match:
        return match.group(1).strip()
    else:
        return text.strip()

# -----------------------------
# ğŸ§  æå–å¹¶æ¸…é™¤ç‚¹èµæ•°
# -----------------------------
def extract_and_clean_likes(text):
    if not isinstance(text, str):
        return "", 1

    # åŒ¹é…å„ç§æ ¼å¼çš„ã€Œç‚¹èµæ•°ã€ç»“æ„ï¼Œå¹¶æå–æ•°å­—
    like_pattern = r'(?:ç‚¹èµæ•°[:ï¼š]\s*(\d+)|ï¼ˆç‚¹èµæ•°[:ï¼š]\s*(\d+)ï¼‰|ã€ç‚¹èµæ•°[:ï¼š]\s*(\d+)ã€‘)'

    like_match = re.search(like_pattern, text)

    if like_match:
        # æå–ç‚¹èµæ•°
        like_count = 1
        for group in like_match.groups():
            if group:
                like_count = int(group)
                break
        # åˆ é™¤æ•´ä¸ªç‚¹èµç»“æ„
        clean_text = re.sub(like_pattern, '', text).strip()
        return clean_text, like_count
    else:
        return text.strip(), 1

# -----------------------------
# ğŸ“ è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…è·¯å¾„ä¿®æ”¹ï¼‰
# -----------------------------
file_path = script_dir.parent / 'result' / 'processed_tweets_final.csv'
file_dir = file_path.parent

# âœ… ç¬¬ä¸€æ­¥ï¼šåŠ è½½å¹¶æ¸…æ´—â€œä¸è¯é¢˜ç›¸å…³çš„è¯„è®ºâ€æ•°æ®

df_original = pd.read_csv(file_path, on_bad_lines='skip', encoding='utf-8-sig')

if 'å¾®åšå†…å®¹' not in df_original.columns:
    if len(df_original.columns) == 4:
        df_original.columns = ['å¾®åšå†…å®¹', 'ä¸è¯é¢˜ç›¸å…³çš„è¯„è®º', 'ä¸è¯é¢˜æ— å…³çš„è¯„è®º', 'ç‚¹èµæ•°']
    elif len(df_original.columns) == 3:
        df_original.columns = ['å¾®åšå†…å®¹', 'ä¸è¯é¢˜ç›¸å…³çš„è¯„è®º', 'ä¸è¯é¢˜æ— å…³çš„è¯„è®º']

# âœ… ä¿ç•™åŸå§‹å¾®åšå†…å®¹å’Œç‚¹èµæ•°
original_weibo_df = df_original[['å¾®åšå†…å®¹', 'ç‚¹èµæ•°']].copy()
original_weibo_df['å¾®åšå†…å®¹'] = original_weibo_df['å¾®åšå†…å®¹'].astype(str).apply(clean_comment_prefix)
original_weibo_df.dropna(subset=['å¾®åšå†…å®¹'], inplace=True)
original_weibo_df = original_weibo_df[original_weibo_df['å¾®åšå†…å®¹'] != ""].reset_index(drop=True)

# âœ… æ¸…æ´—â€œä¸è¯é¢˜ç›¸å…³çš„è¯„è®ºâ€åˆ—
related_comments = []
likes_list = []

for comment_block in df_original['ä¸è¯é¢˜ç›¸å…³çš„è¯„è®º'].dropna():
    # æŒ‰ \n æˆ– \\ åˆ†å‰²æ¯æ¡è¯„è®º
    lines = re.split(r'\n|\\', comment_block)
    for line in lines:
        cleaned_line, like_count = extract_and_clean_likes(line)
        if cleaned_line:
            related_comments.append(cleaned_line)
            likes_list.append(like_count)

df_related_comments = pd.DataFrame({
    'å¾®åšå†…å®¹': related_comments,
    'ç‚¹èµæ•°': likes_list
})

# âœ… åˆå¹¶åŸå§‹å¾®åšå†…å®¹ å’Œ ç›¸å…³è¯„è®º
combined_df = pd.concat([original_weibo_df, df_related_comments], ignore_index=True)

# âœ… åˆ é™¤æ— æ•ˆå†…å®¹ï¼ˆçº¯æ•°å­—ã€æ‹¬å·ç­‰ï¼‰
def is_invalid_content(text):
    text = str(text).strip()
    pattern = r'^[\d\s\)\]\}\{\}]*$|^$|^[ï¼ˆ$].*[ï¼‰$]$'
    return bool(re.match(pattern, text))

combined_df['å¾®åšå†…å®¹'] = combined_df['å¾®åšå†…å®¹'].str.strip()
invalid_mask = combined_df['å¾®åšå†…å®¹'].apply(is_invalid_content)
combined_df = combined_df[~invalid_mask].reset_index(drop=True)

# âœ… å»é‡å¤„ç†ï¼šä¿ç•™ç‚¹èµæ•°é«˜çš„è®°å½•
cleaned_rows = {}
for _, row in combined_df.iterrows():
    content = row['å¾®åšå†…å®¹'].strip()
    likes = int(row['ç‚¹èµæ•°'])
    if content not in cleaned_rows:
        cleaned_rows[content] = row
    else:
        if likes > int(cleaned_rows[content]['ç‚¹èµæ•°']):
            cleaned_rows[content] = row

new_df = pd.DataFrame(list(cleaned_rows.values()))
new_df = new_df[['å¾®åšå†…å®¹', 'ç‚¹èµæ•°']]
new_df['ç‚¹èµæ•°'] = new_df['ç‚¹èµæ•°'].astype(int)

# âœ… åˆ é™¤å¾®åšå†…å®¹ä¸­çš„ç©ºæ ¼å’Œæ‹¬å·
new_df['å¾®åšå†…å®¹'] = new_df['å¾®åšå†…å®¹'].apply(lambda x: re.sub(r'\s+|[\(\)]', '', x))

# âœ… åˆ é™¤å¼€å¤´æ˜¯â€œæ–‡å­—åŠ ï¼šâ€å’Œç»“å°¾æ˜¯â€œLåŠ ä¸å®šé•¿æ–‡å­—åŠ çš„å¾®åšâ€çš„å†…å®¹
def clean_specific_content(text):
    # åˆ é™¤å¼€å¤´æ˜¯â€œæ–‡å­—åŠ ï¼šâ€çš„å†…å®¹
    text = re.sub(r'^[^ï¼š]+ï¼š', '', text)
    # åˆ é™¤ç»“å°¾æ˜¯â€œLåŠ ä¸å®šé•¿æ–‡å­—åŠ çš„å¾®åšâ€çš„å†…å®¹
    text = re.sub(r'L[^çš„]*çš„å¾®åš$', '', text)
    return text.strip()

new_df['å¾®åšå†…å®¹'] = new_df['å¾®åšå†…å®¹'].apply(clean_specific_content)

# âœ… ä¿å­˜ä¸­é—´æ¸…æ´—åçš„ CSV
cleaned_output_path = file_dir / 'cleaned_related_comments_with_likes.csv'
new_df.to_csv(cleaned_output_path, index=False, encoding='utf-8-sig')
print(f"âœ… å·²æ¸…æ´—å¹¶ä¿å­˜å¸¦ç‚¹èµæ•°çš„ç›¸å…³è¯„è®ºè‡³ï¼š{cleaned_output_path}")

# âœ… ç¬¬ä¸‰æ­¥ï¼šæƒ…æ„Ÿåˆ†æ
sentiments = []
scores = []

for index, row in new_df.iterrows():
    text = row['å¾®åšå†…å®¹']
    sentiment, score = predict_sentiment(text)
    sentiments.append(sentiment)
    scores.append(score)

new_df['æƒ…æ„Ÿç±»åˆ«'] = sentiments
new_df['æƒ…æ„Ÿå¾—åˆ†'] = scores
new_df['æƒ…æ„Ÿç±»åˆ«'] = new_df['æƒ…æ„Ÿç±»åˆ«'].astype('category')

# âœ… ç¬¬å››æ­¥ï¼šLDA ä¸»é¢˜å»ºæ¨¡
texts = new_df['å¾®åšå†…å®¹'].dropna().tolist()
lda_model, topic_distributions = perform_lda(texts, num_topics=10)

# æ‰“å°æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯åŠå…¶æƒé‡
for i in range(lda_model.num_topics):
    topic_keywords = lda_model.print_topic(i)  # æå–æ¯ä¸ªä¸»é¢˜çš„å‰10ä¸ªå…³é”®è¯åŠå…¶æƒé‡
    print(f"ä¸»é¢˜ {i}: {topic_keywords}")

# å°†ä¸»é¢˜åˆ†å¸ƒæ·»åŠ åˆ° DataFrame ä¸­
topic_columns = [f"ä¸»é¢˜_{i}" for i in range(lda_model.num_topics)]
for idx, topic_dist in enumerate(topic_distributions):
    new_df.loc[idx, topic_columns] = topic_dist

# âœ… ç¬¬äº”æ­¥ï¼šå…³é”®è¯æå–ï¼ˆTF-IDFï¼‰å¹¶è¿‡æ»¤æƒ…æ„Ÿå€¾å‘ä¸ºç§¯æçš„å…³é”®è¯
def extract_positive_keywords_tfidf(texts, top_n=20, excluded_keywords=None):
    if excluded_keywords is None:
        excluded_keywords = set()
    valid_texts = [str(text) for text in texts if pd.notna(text) and str(text).strip() != ""]
    if not valid_texts:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬å¯ç”¨äºå…³é”®è¯æå–")
        return []

    vectorizer = TfidfVectorizer(tokenizer=chinese_tokenize, stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(valid_texts)
    feature_array = vectorizer.get_feature_names_out()
    tfidf_sorting = tfidf_matrix.toarray().sum(axis=0).argsort()[::-1]
    top_keywords = [feature_array[i] for i in tfidf_sorting[:top_n]]

    # è¿‡æ»¤æ‰ç‰¹å®šçš„å…³é”®è¯
    filtered_keywords = [keyword for keyword in top_keywords if keyword not in excluded_keywords]

    # è¿‡æ»¤å‡ºæƒ…æ„Ÿå€¾å‘ä¸ºç§¯æçš„å…³é”®è¯
    positive_keywords = set()  # ä½¿ç”¨é›†åˆå»é‡
    for keyword in filtered_keywords:
        sentiment, _ = predict_sentiment(keyword)
        if sentiment == 'æ­£é¢':
            positive_keywords.add(keyword)

    # å¦‚æœè¿‡æ»¤åå…³é”®è¯æ•°é‡ä¸è¶³ï¼Œè¡¥å……å…¶ä»–é«˜é¢‘å…³é”®è¯
    if len(positive_keywords) < top_n:
        other_keywords = [feature_array[i] for i in tfidf_sorting if feature_array[i] not in excluded_keywords]
        for keyword in other_keywords:
            sentiment, _ = predict_sentiment(keyword)
            if sentiment == 'æ­£é¢' and keyword not in positive_keywords:
                positive_keywords.add(keyword)
            if len(positive_keywords) >= top_n:
                break

    # å°†é›†åˆè½¬æ¢ä¸ºåˆ—è¡¨å¹¶è¿”å›
    return list(positive_keywords)

# å®šä¹‰éœ€è¦æ’é™¤çš„å…³é”®è¯
excluded_keywords = {"èåœ", "æ­¦æ±‰", "ç‰¹æ–¯æ‹‰", "å¸æœº","é¦™æ¸¯","è¿™ä¸ª","è¿˜æ˜¯","é©¾é©¶","2025","å…¬å¸","ç™¾åº¦","ç°åœ¨","ç°åœ¨"}

# æå–é«˜é¢‘ç§¯æå…³é”®è¯
top_keywords = extract_positive_keywords_tfidf(new_df['å¾®åšå†…å®¹'], top_n=20, excluded_keywords=excluded_keywords)
print("ğŸ“Œ é«˜é¢‘ç§¯æå…³é”®è¯ï¼š", top_keywords)

# åœ¨ DataFrame ä¸­æ·»åŠ å…³é”®è¯åˆ—
for keyword in top_keywords:
    new_df[f"å…³é”®è¯_{keyword}"] = new_df['å¾®åšå†…å®¹'].str.contains(keyword, case=False).fillna(0).astype(int)

# âœ… ç¬¬å…­æ­¥ï¼šç”Ÿæˆè¯äº‘å›¾
all_words = " ".join([" ".join(chinese_tokenize(text)) for text in new_df['å¾®åšå†…å®¹']])
wc = WordCloud(width=800, height=600, background_color='white', font_path='simhei.ttf').generate(all_words)

plt.figure(figsize=(10, 6))
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.title("å¾®åšè¯„è®ºè¯äº‘å›¾")
plt.show()

# âœ… ç¬¬ä¸ƒæ­¥ï¼šè®¡ç®—æƒ…æ„Ÿå¾—åˆ†çš„åŠ æƒå¹³å‡å€¼
new_df['æƒ…æ„Ÿå¾—åˆ†åŠ æƒ'] = new_df['æƒ…æ„Ÿå¾—åˆ†'] * new_df['ç‚¹èµæ•°']
weighted_sentiment_score = new_df['æƒ…æ„Ÿå¾—åˆ†åŠ æƒ'].sum() / new_df['ç‚¹èµæ•°'].sum()
print(f"ğŸ“Œ æƒ…æ„Ÿå¾—åˆ†çš„åŠ æƒå¹³å‡å€¼ï¼š{weighted_sentiment_score:.4f}")

# âœ… ç¬¬å…«æ­¥ï¼šä¿å­˜æœ€ç»ˆç»“æœåˆ° CSV
output_file = file_dir / 'analyzed_related_comments_with_features_and_likes.csv'
new_df.to_csv(output_file, index=False, encoding='utf-8-sig')
print(f"âœ… æ‰€æœ‰åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ {output_file}")